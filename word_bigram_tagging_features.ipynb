{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fakenewsutilities as fns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read the data, initial data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1161040537207463936</td>\n",
       "      <td>'The Endangered Species Act saved the bald eag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1176360756239118342</td>\n",
       "      <td>'Interesting concept  impeach first, find fact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1099036648573145088</td>\n",
       "      <td>'#BuildTheWall #DeportThemAll</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1092915693203480577</td>\n",
       "      <td>'Why would the MEXICAN GOVT fund this? Who are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1149038450668187654</td>\n",
       "      <td>'Sweden Announces Plan To Get 100% Of Energy F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1161040537207463936  'The Endangered Species Act saved the bald eag...   \n",
       "1  1176360756239118342  'Interesting concept  impeach first, find fact...   \n",
       "2  1099036648573145088                     '#BuildTheWall #DeportThemAll    \n",
       "3  1092915693203480577  'Why would the MEXICAN GOVT fund this? Who are...   \n",
       "4  1149038450668187654  'Sweden Announces Plan To Get 100% Of Energy F...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_labeled.csv')\n",
    "#I didn't remove all punctuation at first step, in case we may include punctuation in generating features.\n",
    "df_clean = fns.wash_pandas_str(df)\n",
    "df_clean.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tweets and lables, split the data into 70% training data and 30% test data\n",
    "X = df_clean.iloc[:,0:2]\n",
    "Y = df_clean.iloc[:,2]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 45, test_size  = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the training data by using Naive Bayes    \n",
    "# I tested with 500, 1000, 1500, 2000, 2500 and 3000 high frequency words, \n",
    "# by comparing the accuracy and running time, I decided to use 2000 high frequency words.\n",
    "train_df, fake_prob_prior= fns.naive_bayes_train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt_in_true</th>\n",
       "      <th>cnt_in_fake</th>\n",
       "      <th>freq_true</th>\n",
       "      <th>freq_fake</th>\n",
       "      <th>total_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>trump</td>\n",
       "      <td>8904</td>\n",
       "      <td>12574</td>\n",
       "      <td>0.082991</td>\n",
       "      <td>0.249020</td>\n",
       "      <td>21478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>us</td>\n",
       "      <td>4488</td>\n",
       "      <td>2998</td>\n",
       "      <td>0.041831</td>\n",
       "      <td>0.059373</td>\n",
       "      <td>7486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>illegal</td>\n",
       "      <td>5838</td>\n",
       "      <td>418</td>\n",
       "      <td>0.054414</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>6256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>gun</td>\n",
       "      <td>5770</td>\n",
       "      <td>432</td>\n",
       "      <td>0.053780</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>6202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>new</td>\n",
       "      <td>3162</td>\n",
       "      <td>2900</td>\n",
       "      <td>0.029472</td>\n",
       "      <td>0.057433</td>\n",
       "      <td>6062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  cnt_in_true  cnt_in_fake  freq_true  freq_fake  total_cnt\n",
       "22     trump         8904        12574   0.082991   0.249020      21478\n",
       "186       us         4488         2998   0.041831   0.059373       7486\n",
       "28   illegal         5838          418   0.054414   0.008278       6256\n",
       "607      gun         5770          432   0.053780   0.008555       6202\n",
       "4        new         3162         2900   0.029472   0.057433       6062"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fns.plot_word_map(train_df,word_count = 50, xlimit = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see, people sent fake tweets and true tweets have different wording habits.\n",
    "We can calculating the frequency of individual words in true and fake tweets.\n",
    "\n",
    "Then we can use this word frequency as feature.\n",
    "\n",
    "Future more, we can also use bigram frequency as feature.\n",
    "\n",
    "After tagging the bigram, we can use bigram-tagging frequency as feature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Features  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Word frequency feature part :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tested, accuracy 0.932000\n",
      "2000 tested, accuracy 0.929500\n",
      "3000 tested, accuracy 0.929000\n",
      "4000 tested, accuracy 0.929500\n",
      "5000 tested, accuracy 0.930600\n",
      "6000 tested, accuracy 0.933167\n",
      "7000 tested, accuracy 0.932143\n",
      "8000 tested, accuracy 0.932750\n",
      "9000 tested, accuracy 0.932889\n",
      "10000 tested, accuracy 0.931500\n",
      "11000 tested, accuracy 0.931364\n",
      "12000 tested, accuracy 0.931167\n",
      "13000 tested, accuracy 0.930615\n",
      "14000 tested, accuracy 0.931143\n",
      "15000 tested, accuracy 0.930133\n",
      "16000 tested, accuracy 0.930562\n",
      "17000 tested, accuracy 0.930000\n",
      "18000 tested, accuracy 0.930944\n",
      "19000 tested, accuracy 0.931632\n",
      "20000 tested, accuracy 0.931500\n",
      "21000 tested, accuracy 0.932190\n",
      "22000 tested, accuracy 0.932273\n",
      "23000 tested, accuracy 0.932087\n",
      "24000 tested, accuracy 0.932125\n",
      "25000 tested, accuracy 0.932440\n",
      "26000 tested, accuracy 0.931808\n",
      "27000 tested, accuracy 0.932593\n",
      "28000 tested, accuracy 0.933143\n",
      "29000 tested, accuracy 0.933138\n",
      "30000 tested, accuracy 0.932900\n",
      "31000 tested, accuracy 0.932710\n",
      "32000 tested, accuracy 0.932406\n",
      "33000 tested, accuracy 0.932333\n",
      "34000 tested, accuracy 0.932206\n",
      "35000 tested, accuracy 0.932314\n",
      "36000 tested, accuracy 0.932056\n",
      "37000 tested, accuracy 0.932000\n",
      "38000 tested, accuracy 0.932026\n",
      "39000 tested, accuracy 0.931949\n",
      "40000 tested, accuracy 0.932075\n",
      "41000 tested, accuracy 0.932195\n",
      "42000 tested, accuracy 0.932095\n",
      "43000 tested, accuracy 0.931837\n",
      "44000 tested, accuracy 0.932068\n",
      "45000 tested, accuracy 0.932044\n",
      "46000 tested, accuracy 0.931913\n",
      "47000 tested, accuracy 0.932255\n",
      "48000 tested, accuracy 0.932000\n",
      "49000 tested, accuracy 0.932102\n",
      "50000 tested, accuracy 0.932120\n",
      "51000 tested, accuracy 0.932196\n",
      "52000 tested, accuracy 0.932269\n",
      "53000 tested, accuracy 0.932170\n",
      "54000 tested, accuracy 0.932278\n",
      "55000 tested, accuracy 0.932145\n",
      "56000 tested, accuracy 0.932089\n",
      "57000 tested, accuracy 0.932246\n",
      "58000 tested, accuracy 0.932190\n",
      "59000 tested, accuracy 0.932390\n",
      "60000 tested, accuracy 0.932467\n",
      "61000 tested, accuracy 0.932311\n",
      "62000 tested, accuracy 0.932274\n",
      "63000 tested, accuracy 0.932365\n",
      "64000 tested, accuracy 0.932219\n",
      "65000 tested, accuracy 0.932277\n",
      "66000 tested, accuracy 0.932470\n",
      "67000 tested, accuracy 0.932627\n",
      "68000 tested, accuracy 0.932500\n",
      "69000 tested, accuracy 0.932362\n",
      "70000 tested, accuracy 0.932214\n",
      "71000 tested, accuracy 0.932211\n",
      "72000 tested, accuracy 0.932292\n",
      "73000 tested, accuracy 0.932260\n",
      "74000 tested, accuracy 0.932216\n",
      "75000 tested, accuracy 0.932147\n",
      "76000 tested, accuracy 0.932105\n",
      "77000 tested, accuracy 0.932091\n",
      "78000 tested, accuracy 0.932192\n",
      "79000 tested, accuracy 0.932139\n",
      "80000 tested, accuracy 0.932312\n",
      "81000 tested, accuracy 0.932296\n",
      "82000 tested, accuracy 0.932329\n",
      "83000 tested, accuracy 0.932470\n",
      "84000 tested, accuracy 0.932417\n",
      "85000 tested, accuracy 0.932353\n",
      "86000 tested, accuracy 0.932279\n",
      "87000 tested, accuracy 0.932207\n",
      "88000 tested, accuracy 0.932318\n",
      "89000 tested, accuracy 0.932247\n",
      "90000 tested, accuracy 0.932200\n",
      "91000 tested, accuracy 0.932264\n",
      "92000 tested, accuracy 0.932207\n",
      "93000 tested, accuracy 0.932108\n",
      "94000 tested, accuracy 0.931989\n",
      "95000 tested, accuracy 0.931916\n",
      "96000 tested, accuracy 0.932000\n",
      "97000 tested, accuracy 0.932000\n",
      "98000 tested, accuracy 0.932000\n",
      "99000 tested, accuracy 0.931919\n",
      "100000 tested, accuracy 0.931890\n",
      "101000 tested, accuracy 0.931911\n",
      "102000 tested, accuracy 0.932010\n",
      "103000 tested, accuracy 0.932058\n",
      "104000 tested, accuracy 0.931952\n",
      "105000 tested, accuracy 0.932000\n",
      "106000 tested, accuracy 0.932000\n",
      "107000 tested, accuracy 0.931916\n",
      "108000 tested, accuracy 0.932028\n",
      "109000 tested, accuracy 0.931972\n",
      "110000 tested, accuracy 0.931891\n",
      "111000 tested, accuracy 0.931829\n",
      "112000 tested, accuracy 0.931759\n",
      "113000 tested, accuracy 0.931823\n",
      "114000 tested, accuracy 0.931877\n",
      "115000 tested, accuracy 0.931930\n",
      "116000 tested, accuracy 0.932000\n",
      "117000 tested, accuracy 0.932026\n",
      "118000 tested, accuracy 0.931966\n",
      "119000 tested, accuracy 0.931950\n",
      "120000 tested, accuracy 0.932025\n",
      "121000 tested, accuracy 0.932074\n",
      "122000 tested, accuracy 0.932057\n",
      "123000 tested, accuracy 0.932081\n",
      "124000 tested, accuracy 0.932048\n",
      "125000 tested, accuracy 0.932072\n",
      "126000 tested, accuracy 0.932103\n",
      "127000 tested, accuracy 0.932126\n",
      "128000 tested, accuracy 0.932133\n",
      "129000 tested, accuracy 0.932171\n",
      "130000 tested, accuracy 0.932246\n",
      "131000 tested, accuracy 0.932206\n",
      "132000 tested, accuracy 0.932182\n",
      "133000 tested, accuracy 0.932113\n",
      "134000 tested, accuracy 0.932201\n",
      "135000 tested, accuracy 0.932081\n",
      "136000 tested, accuracy 0.932191\n",
      "137000 tested, accuracy 0.932146\n",
      "138000 tested, accuracy 0.932159\n",
      "139000 tested, accuracy 0.932144\n",
      "140000 tested, accuracy 0.932207\n",
      "141000 tested, accuracy 0.932220\n",
      "142000 tested, accuracy 0.932204\n",
      "143000 tested, accuracy 0.932238\n",
      "144000 tested, accuracy 0.932222\n",
      "145000 tested, accuracy 0.932214\n",
      "146000 tested, accuracy 0.932178\n",
      "147000 tested, accuracy 0.932218\n",
      "148000 tested, accuracy 0.932196\n",
      "149000 tested, accuracy 0.932208\n",
      "150000 tested, accuracy 0.932207\n",
      "151000 tested, accuracy 0.932126\n",
      "152000 tested, accuracy 0.932184\n",
      "153000 tested, accuracy 0.932255\n",
      "154000 tested, accuracy 0.932188\n",
      "155000 tested, accuracy 0.932181\n",
      "156000 tested, accuracy 0.932231\n",
      "157000 tested, accuracy 0.932261\n"
     ]
    }
   ],
   "source": [
    "# get word frequency feature for svm\n",
    "word_frequency_feature_train = fns.naive_bayes_generate_feature(train_df, fake_prob_prior, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Bigram frequency feature part :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the training data by using Naive Bayes \n",
    "# I tested with 500, 1000, 1500, 2000, 2500 and 3000 high frequency bigrams, \n",
    "# by comparing the accuracy and running time, I decided to use 2000 high frequency bigrams.\n",
    "train_df_bigrm, fake_prob_prior = fns.naive_bayes_bigrm_train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tested, accuracy 0.894000\n",
      "2000 tested, accuracy 0.883000\n",
      "3000 tested, accuracy 0.886667\n",
      "4000 tested, accuracy 0.887000\n",
      "5000 tested, accuracy 0.887800\n",
      "6000 tested, accuracy 0.890000\n",
      "7000 tested, accuracy 0.891000\n",
      "8000 tested, accuracy 0.889875\n",
      "9000 tested, accuracy 0.889333\n",
      "10000 tested, accuracy 0.888700\n",
      "11000 tested, accuracy 0.889273\n",
      "12000 tested, accuracy 0.888667\n",
      "13000 tested, accuracy 0.888154\n",
      "14000 tested, accuracy 0.888786\n",
      "15000 tested, accuracy 0.886867\n",
      "16000 tested, accuracy 0.887563\n",
      "17000 tested, accuracy 0.887824\n",
      "18000 tested, accuracy 0.887833\n",
      "19000 tested, accuracy 0.888316\n",
      "20000 tested, accuracy 0.887950\n",
      "21000 tested, accuracy 0.887952\n",
      "22000 tested, accuracy 0.888545\n",
      "23000 tested, accuracy 0.888609\n",
      "24000 tested, accuracy 0.888583\n",
      "25000 tested, accuracy 0.888960\n",
      "26000 tested, accuracy 0.888615\n",
      "27000 tested, accuracy 0.889037\n",
      "28000 tested, accuracy 0.889250\n",
      "29000 tested, accuracy 0.889483\n",
      "30000 tested, accuracy 0.889500\n",
      "31000 tested, accuracy 0.889419\n",
      "32000 tested, accuracy 0.888906\n",
      "33000 tested, accuracy 0.888909\n",
      "34000 tested, accuracy 0.889059\n",
      "35000 tested, accuracy 0.889029\n",
      "36000 tested, accuracy 0.888528\n",
      "37000 tested, accuracy 0.888595\n",
      "38000 tested, accuracy 0.888947\n",
      "39000 tested, accuracy 0.888436\n",
      "40000 tested, accuracy 0.888850\n",
      "41000 tested, accuracy 0.889195\n",
      "42000 tested, accuracy 0.888857\n",
      "43000 tested, accuracy 0.888977\n",
      "44000 tested, accuracy 0.889023\n",
      "45000 tested, accuracy 0.889044\n",
      "46000 tested, accuracy 0.889435\n",
      "47000 tested, accuracy 0.889532\n",
      "48000 tested, accuracy 0.889521\n",
      "49000 tested, accuracy 0.889286\n",
      "50000 tested, accuracy 0.889220\n",
      "51000 tested, accuracy 0.889373\n",
      "52000 tested, accuracy 0.889423\n",
      "53000 tested, accuracy 0.889264\n",
      "54000 tested, accuracy 0.889241\n",
      "55000 tested, accuracy 0.888945\n",
      "56000 tested, accuracy 0.889036\n",
      "57000 tested, accuracy 0.888965\n",
      "58000 tested, accuracy 0.889052\n",
      "59000 tested, accuracy 0.889153\n",
      "60000 tested, accuracy 0.889333\n",
      "61000 tested, accuracy 0.889377\n",
      "62000 tested, accuracy 0.889403\n",
      "63000 tested, accuracy 0.889603\n",
      "64000 tested, accuracy 0.889641\n",
      "65000 tested, accuracy 0.889538\n",
      "66000 tested, accuracy 0.889621\n",
      "67000 tested, accuracy 0.889821\n",
      "68000 tested, accuracy 0.889853\n",
      "69000 tested, accuracy 0.889768\n",
      "70000 tested, accuracy 0.889514\n",
      "71000 tested, accuracy 0.889338\n",
      "72000 tested, accuracy 0.889583\n",
      "73000 tested, accuracy 0.889329\n",
      "74000 tested, accuracy 0.889108\n",
      "75000 tested, accuracy 0.888933\n",
      "76000 tested, accuracy 0.888789\n",
      "77000 tested, accuracy 0.888779\n",
      "78000 tested, accuracy 0.888859\n",
      "79000 tested, accuracy 0.888722\n",
      "80000 tested, accuracy 0.888850\n",
      "81000 tested, accuracy 0.888667\n",
      "82000 tested, accuracy 0.888756\n",
      "83000 tested, accuracy 0.888904\n",
      "84000 tested, accuracy 0.888976\n",
      "85000 tested, accuracy 0.889024\n",
      "86000 tested, accuracy 0.888942\n",
      "87000 tested, accuracy 0.888966\n",
      "88000 tested, accuracy 0.889034\n",
      "89000 tested, accuracy 0.889034\n",
      "90000 tested, accuracy 0.889033\n",
      "91000 tested, accuracy 0.889088\n",
      "92000 tested, accuracy 0.888978\n",
      "93000 tested, accuracy 0.888892\n",
      "94000 tested, accuracy 0.888915\n",
      "95000 tested, accuracy 0.888947\n",
      "96000 tested, accuracy 0.888906\n",
      "97000 tested, accuracy 0.889021\n",
      "98000 tested, accuracy 0.888969\n",
      "99000 tested, accuracy 0.889010\n",
      "100000 tested, accuracy 0.889030\n",
      "101000 tested, accuracy 0.889119\n",
      "102000 tested, accuracy 0.889147\n",
      "103000 tested, accuracy 0.889175\n",
      "104000 tested, accuracy 0.889240\n",
      "105000 tested, accuracy 0.889305\n",
      "106000 tested, accuracy 0.889349\n",
      "107000 tested, accuracy 0.889224\n",
      "108000 tested, accuracy 0.889194\n",
      "109000 tested, accuracy 0.889284\n",
      "110000 tested, accuracy 0.889200\n",
      "111000 tested, accuracy 0.889225\n",
      "112000 tested, accuracy 0.889179\n",
      "113000 tested, accuracy 0.889204\n",
      "114000 tested, accuracy 0.889298\n",
      "115000 tested, accuracy 0.889243\n",
      "116000 tested, accuracy 0.889241\n",
      "117000 tested, accuracy 0.889274\n",
      "118000 tested, accuracy 0.889314\n",
      "119000 tested, accuracy 0.889370\n",
      "120000 tested, accuracy 0.889342\n",
      "121000 tested, accuracy 0.889455\n",
      "122000 tested, accuracy 0.889484\n",
      "123000 tested, accuracy 0.889407\n",
      "124000 tested, accuracy 0.889524\n",
      "125000 tested, accuracy 0.889520\n",
      "126000 tested, accuracy 0.889516\n",
      "127000 tested, accuracy 0.889512\n",
      "128000 tested, accuracy 0.889531\n",
      "129000 tested, accuracy 0.889496\n",
      "130000 tested, accuracy 0.889408\n",
      "131000 tested, accuracy 0.889397\n",
      "132000 tested, accuracy 0.889477\n",
      "133000 tested, accuracy 0.889451\n",
      "134000 tested, accuracy 0.889627\n",
      "135000 tested, accuracy 0.889548\n",
      "136000 tested, accuracy 0.889632\n",
      "137000 tested, accuracy 0.889547\n",
      "138000 tested, accuracy 0.889688\n",
      "139000 tested, accuracy 0.889647\n",
      "140000 tested, accuracy 0.889693\n",
      "141000 tested, accuracy 0.889702\n",
      "142000 tested, accuracy 0.889627\n",
      "143000 tested, accuracy 0.889622\n",
      "144000 tested, accuracy 0.889687\n",
      "145000 tested, accuracy 0.889752\n",
      "146000 tested, accuracy 0.889658\n",
      "147000 tested, accuracy 0.889748\n",
      "148000 tested, accuracy 0.889791\n",
      "149000 tested, accuracy 0.889826\n",
      "150000 tested, accuracy 0.889780\n",
      "151000 tested, accuracy 0.889768\n",
      "152000 tested, accuracy 0.889789\n",
      "153000 tested, accuracy 0.889889\n",
      "154000 tested, accuracy 0.889805\n",
      "155000 tested, accuracy 0.889768\n",
      "156000 tested, accuracy 0.889827\n",
      "157000 tested, accuracy 0.889841\n"
     ]
    }
   ],
   "source": [
    "# generate bigram frequency feature for svm\n",
    "bigrm_frequency_feature_train = fns.naive_bayes_generate_feature_bigrm(train_df_bigrm, fake_prob_prior, X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Bigram- tagging frequency feature part: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 processed\n",
      "2000 processed\n",
      "3000 processed\n",
      "4000 processed\n",
      "5000 processed\n",
      "6000 processed\n",
      "7000 processed\n",
      "8000 processed\n",
      "9000 processed\n",
      "10000 processed\n",
      "11000 processed\n",
      "12000 processed\n",
      "13000 processed\n",
      "14000 processed\n",
      "15000 processed\n",
      "16000 processed\n",
      "17000 processed\n",
      "18000 processed\n",
      "19000 processed\n",
      "20000 processed\n",
      "21000 processed\n",
      "22000 processed\n",
      "23000 processed\n",
      "24000 processed\n",
      "25000 processed\n",
      "26000 processed\n",
      "27000 processed\n",
      "28000 processed\n",
      "29000 processed\n",
      "30000 processed\n",
      "31000 processed\n",
      "32000 processed\n",
      "33000 processed\n",
      "34000 processed\n",
      "35000 processed\n",
      "36000 processed\n",
      "37000 processed\n",
      "38000 processed\n",
      "39000 processed\n",
      "40000 processed\n",
      "41000 processed\n",
      "42000 processed\n",
      "43000 processed\n",
      "44000 processed\n",
      "45000 processed\n",
      "46000 processed\n",
      "47000 processed\n",
      "48000 processed\n",
      "49000 processed\n",
      "50000 processed\n",
      "51000 processed\n",
      "52000 processed\n",
      "53000 processed\n",
      "54000 processed\n",
      "55000 processed\n",
      "56000 processed\n",
      "57000 processed\n",
      "58000 processed\n",
      "59000 processed\n",
      "60000 processed\n",
      "61000 processed\n",
      "62000 processed\n",
      "63000 processed\n",
      "64000 processed\n",
      "65000 processed\n",
      "66000 processed\n",
      "67000 processed\n",
      "68000 processed\n",
      "69000 processed\n",
      "70000 processed\n",
      "71000 processed\n",
      "72000 processed\n",
      "73000 processed\n",
      "74000 processed\n",
      "75000 processed\n",
      "76000 processed\n",
      "77000 processed\n",
      "78000 processed\n",
      "79000 processed\n",
      "80000 processed\n",
      "81000 processed\n",
      "82000 processed\n",
      "83000 processed\n",
      "84000 processed\n",
      "85000 processed\n",
      "86000 processed\n",
      "87000 processed\n",
      "88000 processed\n",
      "89000 processed\n",
      "90000 processed\n",
      "91000 processed\n",
      "92000 processed\n",
      "93000 processed\n",
      "94000 processed\n",
      "95000 processed\n",
      "96000 processed\n",
      "97000 processed\n",
      "98000 processed\n",
      "99000 processed\n",
      "100000 processed\n",
      "101000 processed\n",
      "102000 processed\n",
      "103000 processed\n",
      "104000 processed\n",
      "105000 processed\n",
      "106000 processed\n",
      "107000 processed\n",
      "108000 processed\n",
      "109000 processed\n",
      "110000 processed\n",
      "111000 processed\n",
      "112000 processed\n",
      "113000 processed\n",
      "114000 processed\n",
      "115000 processed\n",
      "116000 processed\n",
      "117000 processed\n",
      "118000 processed\n",
      "119000 processed\n",
      "120000 processed\n",
      "121000 processed\n",
      "122000 processed\n",
      "123000 processed\n",
      "124000 processed\n",
      "125000 processed\n",
      "126000 processed\n",
      "127000 processed\n",
      "128000 processed\n",
      "129000 processed\n",
      "130000 processed\n",
      "131000 processed\n",
      "132000 processed\n",
      "133000 processed\n",
      "134000 processed\n",
      "135000 processed\n",
      "136000 processed\n",
      "137000 processed\n",
      "138000 processed\n",
      "139000 processed\n",
      "140000 processed\n",
      "141000 processed\n",
      "142000 processed\n",
      "143000 processed\n",
      "144000 processed\n",
      "145000 processed\n",
      "146000 processed\n",
      "147000 processed\n",
      "148000 processed\n",
      "149000 processed\n",
      "150000 processed\n",
      "151000 processed\n",
      "152000 processed\n",
      "153000 processed\n",
      "154000 processed\n",
      "155000 processed\n",
      "156000 processed\n",
      "157000 processed\n"
     ]
    }
   ],
   "source": [
    "# Bigram of Tags using SVM\n",
    "train_tagged_bigram_features = fns.tags_bigram_generate_features(X_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Build the mode,use svm from sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 for word frequency feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = fns.conv_array(word_frequency_feature_train)\n",
    "y = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_word = svm.SVC()\n",
    "clf_word.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tested, accuracy 0.940000\n",
      "2000 tested, accuracy 0.937500\n",
      "3000 tested, accuracy 0.934333\n",
      "4000 tested, accuracy 0.936000\n",
      "5000 tested, accuracy 0.934400\n",
      "6000 tested, accuracy 0.934167\n",
      "7000 tested, accuracy 0.933857\n",
      "8000 tested, accuracy 0.934125\n",
      "9000 tested, accuracy 0.933667\n",
      "10000 tested, accuracy 0.933400\n",
      "11000 tested, accuracy 0.933000\n",
      "12000 tested, accuracy 0.933417\n",
      "13000 tested, accuracy 0.934000\n",
      "14000 tested, accuracy 0.934571\n",
      "15000 tested, accuracy 0.934533\n",
      "16000 tested, accuracy 0.934937\n",
      "17000 tested, accuracy 0.934235\n",
      "18000 tested, accuracy 0.934167\n",
      "19000 tested, accuracy 0.934053\n",
      "20000 tested, accuracy 0.934200\n",
      "21000 tested, accuracy 0.933952\n",
      "22000 tested, accuracy 0.933864\n",
      "23000 tested, accuracy 0.933565\n",
      "24000 tested, accuracy 0.932875\n",
      "25000 tested, accuracy 0.933000\n",
      "26000 tested, accuracy 0.932885\n",
      "27000 tested, accuracy 0.932481\n",
      "28000 tested, accuracy 0.932571\n",
      "29000 tested, accuracy 0.932793\n",
      "30000 tested, accuracy 0.932667\n",
      "31000 tested, accuracy 0.932419\n",
      "32000 tested, accuracy 0.932719\n",
      "33000 tested, accuracy 0.932667\n",
      "34000 tested, accuracy 0.932353\n",
      "35000 tested, accuracy 0.932114\n",
      "36000 tested, accuracy 0.932250\n",
      "37000 tested, accuracy 0.932000\n",
      "38000 tested, accuracy 0.932105\n",
      "39000 tested, accuracy 0.932179\n",
      "40000 tested, accuracy 0.932125\n",
      "41000 tested, accuracy 0.932171\n",
      "42000 tested, accuracy 0.932119\n",
      "43000 tested, accuracy 0.931860\n",
      "44000 tested, accuracy 0.931659\n",
      "45000 tested, accuracy 0.931756\n",
      "46000 tested, accuracy 0.931913\n",
      "47000 tested, accuracy 0.932106\n",
      "48000 tested, accuracy 0.932312\n",
      "49000 tested, accuracy 0.932184\n",
      "50000 tested, accuracy 0.932340\n",
      "51000 tested, accuracy 0.932176\n",
      "52000 tested, accuracy 0.932019\n",
      "53000 tested, accuracy 0.932113\n",
      "54000 tested, accuracy 0.931981\n",
      "55000 tested, accuracy 0.932036\n",
      "56000 tested, accuracy 0.932036\n",
      "57000 tested, accuracy 0.932053\n",
      "58000 tested, accuracy 0.932138\n",
      "59000 tested, accuracy 0.932153\n",
      "60000 tested, accuracy 0.932233\n",
      "61000 tested, accuracy 0.932361\n",
      "62000 tested, accuracy 0.932468\n",
      "63000 tested, accuracy 0.932524\n",
      "64000 tested, accuracy 0.932516\n",
      "65000 tested, accuracy 0.932492\n",
      "66000 tested, accuracy 0.932636\n",
      "67000 tested, accuracy 0.932776\n"
     ]
    }
   ],
   "source": [
    "#get word frequency of testing data set\n",
    "word_frequency_feature_test = fns.naive_bayes_generate_feature(train_df, fake_prob_prior, X_test, Y_test)\n",
    "X2 = fns.conv_array(word_frequency_feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 for bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = fns.conv_array(bigrm_frequency_feature_train)\n",
    "y = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bigrm = svm.SVC()\n",
    "clf_bigrm.fit(X3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tested, accuracy 0.905000\n",
      "2000 tested, accuracy 0.899500\n",
      "3000 tested, accuracy 0.899667\n",
      "4000 tested, accuracy 0.894750\n",
      "5000 tested, accuracy 0.895800\n",
      "6000 tested, accuracy 0.895667\n",
      "7000 tested, accuracy 0.895571\n",
      "8000 tested, accuracy 0.894375\n",
      "9000 tested, accuracy 0.894000\n",
      "10000 tested, accuracy 0.894100\n",
      "11000 tested, accuracy 0.893818\n",
      "12000 tested, accuracy 0.895500\n",
      "13000 tested, accuracy 0.895692\n",
      "14000 tested, accuracy 0.895357\n",
      "15000 tested, accuracy 0.894867\n",
      "16000 tested, accuracy 0.895687\n",
      "17000 tested, accuracy 0.894882\n",
      "18000 tested, accuracy 0.894389\n",
      "19000 tested, accuracy 0.894632\n",
      "20000 tested, accuracy 0.894350\n",
      "21000 tested, accuracy 0.893714\n",
      "22000 tested, accuracy 0.893136\n",
      "23000 tested, accuracy 0.892261\n",
      "24000 tested, accuracy 0.891958\n",
      "25000 tested, accuracy 0.892120\n",
      "26000 tested, accuracy 0.891731\n",
      "27000 tested, accuracy 0.891259\n",
      "28000 tested, accuracy 0.891143\n",
      "29000 tested, accuracy 0.890931\n",
      "30000 tested, accuracy 0.890500\n",
      "31000 tested, accuracy 0.890935\n",
      "32000 tested, accuracy 0.890625\n",
      "33000 tested, accuracy 0.890515\n",
      "34000 tested, accuracy 0.890206\n",
      "35000 tested, accuracy 0.889657\n",
      "36000 tested, accuracy 0.889917\n",
      "37000 tested, accuracy 0.889730\n",
      "38000 tested, accuracy 0.889711\n",
      "39000 tested, accuracy 0.889769\n",
      "40000 tested, accuracy 0.889700\n",
      "41000 tested, accuracy 0.889878\n",
      "42000 tested, accuracy 0.890095\n",
      "43000 tested, accuracy 0.889907\n",
      "44000 tested, accuracy 0.889750\n",
      "45000 tested, accuracy 0.889422\n",
      "46000 tested, accuracy 0.889174\n",
      "47000 tested, accuracy 0.888936\n",
      "48000 tested, accuracy 0.888604\n",
      "49000 tested, accuracy 0.888592\n",
      "50000 tested, accuracy 0.888860\n",
      "51000 tested, accuracy 0.888608\n",
      "52000 tested, accuracy 0.888596\n",
      "53000 tested, accuracy 0.888547\n",
      "54000 tested, accuracy 0.888333\n",
      "55000 tested, accuracy 0.888364\n",
      "56000 tested, accuracy 0.888196\n",
      "57000 tested, accuracy 0.888368\n",
      "58000 tested, accuracy 0.888414\n",
      "59000 tested, accuracy 0.888559\n",
      "60000 tested, accuracy 0.888467\n",
      "61000 tested, accuracy 0.888787\n",
      "62000 tested, accuracy 0.888661\n",
      "63000 tested, accuracy 0.888730\n",
      "64000 tested, accuracy 0.888672\n",
      "65000 tested, accuracy 0.888708\n",
      "66000 tested, accuracy 0.888621\n",
      "67000 tested, accuracy 0.888940\n"
     ]
    }
   ],
   "source": [
    "# get bigram frequency testing data set\n",
    "bigrm_frequency_feature_test = fns.naive_bayes_generate_feature_bigrm(train_df_bigrm, fake_prob_prior, X_test, Y_test)\n",
    "X4 = fns.conv_array(bigrm_frequency_feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 for bigram tagging frequency feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tags = svm.SVC(gamma='scale')\n",
    "clf_tags.fit(train_tagged_bigram_features, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 processed\n",
      "2000 processed\n",
      "3000 processed\n",
      "4000 processed\n",
      "5000 processed\n",
      "6000 processed\n",
      "7000 processed\n",
      "8000 processed\n",
      "9000 processed\n",
      "10000 processed\n",
      "11000 processed\n",
      "12000 processed\n",
      "13000 processed\n",
      "14000 processed\n",
      "15000 processed\n",
      "16000 processed\n",
      "17000 processed\n",
      "18000 processed\n",
      "19000 processed\n",
      "20000 processed\n",
      "21000 processed\n",
      "22000 processed\n",
      "23000 processed\n",
      "24000 processed\n",
      "25000 processed\n",
      "26000 processed\n",
      "27000 processed\n",
      "28000 processed\n",
      "29000 processed\n",
      "30000 processed\n",
      "31000 processed\n",
      "32000 processed\n",
      "33000 processed\n",
      "34000 processed\n",
      "35000 processed\n",
      "36000 processed\n",
      "37000 processed\n",
      "38000 processed\n",
      "39000 processed\n",
      "40000 processed\n",
      "41000 processed\n",
      "42000 processed\n",
      "43000 processed\n",
      "44000 processed\n",
      "45000 processed\n",
      "46000 processed\n",
      "47000 processed\n",
      "48000 processed\n",
      "49000 processed\n",
      "50000 processed\n",
      "51000 processed\n",
      "52000 processed\n",
      "53000 processed\n",
      "54000 processed\n",
      "55000 processed\n",
      "56000 processed\n",
      "57000 processed\n",
      "58000 processed\n",
      "59000 processed\n",
      "60000 processed\n",
      "61000 processed\n",
      "62000 processed\n",
      "63000 processed\n",
      "64000 processed\n",
      "65000 processed\n",
      "66000 processed\n",
      "67000 processed\n"
     ]
    }
   ],
   "source": [
    "test_tagged_bigram_features = fns.tags_bigram_generate_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Evaluation the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 for work frequency feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399899441010322"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_nparray = np.array(Y_test)\n",
    "Y_predict = clf_word.predict(X2)\n",
    "\n",
    "clf_word.score(X2, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44479,  1694],\n",
       "       [ 2364, 19085]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the confusion matrix to see how our model worked on test set.  \n",
    "cm_word = confusion_matrix(Y_test, Y_predict)\n",
    "cm_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     46173\n",
      "           1       0.92      0.89      0.90     21449\n",
      "\n",
      "    accuracy                           0.94     67622\n",
      "   macro avg       0.93      0.93      0.93     67622\n",
      "weighted avg       0.94      0.94      0.94     67622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# details of accuracy and recall :\n",
    "print(classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for fake tweets is 92% , for true tweets is better, 95%\n",
    "\n",
    "The recall of fake tweets is 89%, about 11% tweets were wrong predictioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 for bigram frequency feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942799680577327"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_nparray = np.array(Y_test)\n",
    "Y_predict_2 = clf_bigrm.predict(X4)\n",
    "\n",
    "clf_bigrm.score(X4, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44204,  1969],\n",
       "       [ 5180, 16269]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the confusion matrix to see how our model worked on test set.  \n",
    "cm_bigrm = confusion_matrix(Y_test, Y_predict_2)\n",
    "cm_bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     46173\n",
      "           1       0.89      0.76      0.82     21449\n",
      "\n",
      "    accuracy                           0.89     67622\n",
      "   macro avg       0.89      0.86      0.87     67622\n",
      "weighted avg       0.89      0.89      0.89     67622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# details of accuracy and recall :\n",
    "print(classification_report(Y_test, Y_predict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for fake tweets is 89% , for true tweets is better, 90%\n",
    "\n",
    "The recall of fake tweets is 76%, about 24% tweets were wrong predictioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 for tagged bigram frequency feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tags.score(test_tagged_bigram_features, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_3 = clf_tags.predict(test_tagged_bigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix to see how our model worked on test set.  \n",
    "cm_tags = confusion_matrix(Y_test, Y_predict_3)\n",
    "cm_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     46173\n",
      "           1       0.85      0.61      0.71     21449\n",
      "\n",
      "    accuracy                           0.84     67622\n",
      "   macro avg       0.84      0.78      0.80     67622\n",
      "weighted avg       0.84      0.84      0.83     67622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# details of accuracy and recall :\n",
    "print(classification_report(Y_test, Y_predict_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for fake tweets is 80% , for true tweets is a little better, 83%\n",
    "\n",
    "But the recall of fake tweets is not good, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
