{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fakenewsutilities as fns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the data, then clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1161040537207463936</td>\n",
       "      <td>'The Endangered Species Act saved the bald eag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1176360756239118342</td>\n",
       "      <td>'Interesting concept -- impeach first, find fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1099036648573145088</td>\n",
       "      <td>'#BuildTheWall #DeportThemAll</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1092915693203480577</td>\n",
       "      <td>'Why would the MEXICAN GOVT fund this? Who are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1149038450668187654</td>\n",
       "      <td>'Sweden Announces Plan To Get 100% Of Energy F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1161040537207463936  'The Endangered Species Act saved the bald eag...   \n",
       "1  1176360756239118342  'Interesting concept -- impeach first, find fa...   \n",
       "2  1099036648573145088                     '#BuildTheWall #DeportThemAll    \n",
       "3  1092915693203480577  'Why would the MEXICAN GOVT fund this? Who are...   \n",
       "4  1149038450668187654  'Sweden Announces Plan To Get 100% Of Energy F...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "df = pd.read_csv('tweets_labeled.csv')\n",
    "df_clean = fns.wash_pandas_str(df)\n",
    "df_clean.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tweets and lables\n",
    "X = df_clean.iloc[:,0:2]\n",
    "Y = df_clean.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 45, test_size  = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, fake_prob_prior = fns.naive_bayes_bigrm_train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tested, accuracy 0.832000\n",
      "2000 tested, accuracy 0.826500\n",
      "3000 tested, accuracy 0.821333\n",
      "4000 tested, accuracy 0.812750\n",
      "5000 tested, accuracy 0.813200\n",
      "6000 tested, accuracy 0.813000\n",
      "7000 tested, accuracy 0.813000\n",
      "8000 tested, accuracy 0.811125\n",
      "9000 tested, accuracy 0.811333\n",
      "10000 tested, accuracy 0.810200\n",
      "11000 tested, accuracy 0.811000\n",
      "12000 tested, accuracy 0.811833\n",
      "13000 tested, accuracy 0.812231\n",
      "14000 tested, accuracy 0.812786\n",
      "15000 tested, accuracy 0.812533\n",
      "16000 tested, accuracy 0.813438\n",
      "17000 tested, accuracy 0.813294\n",
      "18000 tested, accuracy 0.812611\n",
      "19000 tested, accuracy 0.812474\n",
      "20000 tested, accuracy 0.812500\n",
      "21000 tested, accuracy 0.812190\n",
      "22000 tested, accuracy 0.811636\n",
      "23000 tested, accuracy 0.811174\n",
      "24000 tested, accuracy 0.811375\n",
      "25000 tested, accuracy 0.811960\n",
      "26000 tested, accuracy 0.812038\n",
      "27000 tested, accuracy 0.812111\n",
      "28000 tested, accuracy 0.811357\n",
      "29000 tested, accuracy 0.811103\n",
      "30000 tested, accuracy 0.810833\n",
      "31000 tested, accuracy 0.811419\n",
      "32000 tested, accuracy 0.810906\n",
      "33000 tested, accuracy 0.810636\n",
      "34000 tested, accuracy 0.810824\n",
      "35000 tested, accuracy 0.810914\n",
      "36000 tested, accuracy 0.810667\n",
      "37000 tested, accuracy 0.810946\n",
      "38000 tested, accuracy 0.810974\n",
      "39000 tested, accuracy 0.810692\n",
      "40000 tested, accuracy 0.810700\n",
      "41000 tested, accuracy 0.810951\n",
      "42000 tested, accuracy 0.810500\n",
      "43000 tested, accuracy 0.810558\n",
      "44000 tested, accuracy 0.810114\n",
      "45000 tested, accuracy 0.809689\n",
      "46000 tested, accuracy 0.809261\n",
      "47000 tested, accuracy 0.809447\n",
      "48000 tested, accuracy 0.809292\n",
      "49000 tested, accuracy 0.809388\n",
      "50000 tested, accuracy 0.809720\n",
      "51000 tested, accuracy 0.809373\n",
      "52000 tested, accuracy 0.809404\n",
      "53000 tested, accuracy 0.809132\n",
      "54000 tested, accuracy 0.809019\n",
      "55000 tested, accuracy 0.808800\n",
      "56000 tested, accuracy 0.808571\n",
      "57000 tested, accuracy 0.808807\n",
      "58000 tested, accuracy 0.808897\n",
      "59000 tested, accuracy 0.808797\n",
      "60000 tested, accuracy 0.808717\n",
      "61000 tested, accuracy 0.809508\n",
      "62000 tested, accuracy 0.809339\n",
      "63000 tested, accuracy 0.809349\n",
      "64000 tested, accuracy 0.809344\n",
      "65000 tested, accuracy 0.809308\n",
      "66000 tested, accuracy 0.809333\n",
      "67000 tested, accuracy 0.809687\n"
     ]
    }
   ],
   "source": [
    "bigram_frequency_feature = fns.naive_bayes_generate_feature_bigram(train_df, fake_prob_prior, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, I used the most high frequency 500 bigram as a feature, accuracy is about 80%\n",
    "\n",
    "Because 500 two-words cover much less than 500 words, the accuracy is lower than the word frequency feature in 500 words (88%)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
